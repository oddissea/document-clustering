{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc7c19be-1c1e-4abb-9d15-45db54e9a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51d5225-7b7c-4b83-90a7-276e6e3e17a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d7c36f-586f-4a80-a5c5-662ea545b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aquiles/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aquiles/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/aquiles/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargamos recursos necesarios de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcbc7999-0a5c-48f1-a86f-e938be19887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_archivos(ruta_base):\n",
    "    \"\"\"\n",
    "    Lista todos los archivos en las subcarpetas de la ruta base\n",
    "    \"\"\"\n",
    "    archivos = []\n",
    "    categorias = []\n",
    "    \n",
    "    # Recorremos todas las subcarpetas (categorías)\n",
    "    for categoria in os.listdir(ruta_base):\n",
    "        ruta_categoria = os.path.join(ruta_base, categoria)\n",
    "        \n",
    "        # Verificamos que sea un directorio\n",
    "        if os.path.isdir(ruta_categoria):\n",
    "            # Listamos los archivos en esta categoría\n",
    "            for archivo in os.listdir(ruta_categoria):\n",
    "                ruta_archivo = os.path.join(ruta_categoria, archivo)\n",
    "                \n",
    "                # Verificamos que sea un archivo\n",
    "                if os.path.isfile(ruta_archivo):\n",
    "                    archivos.append(ruta_archivo)\n",
    "                    categorias.append(categoria)\n",
    "    \n",
    "    return archivos, categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35956594-a18d-42b5-90aa-3085c3f44ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_cabecera_firma(texto):\n",
    "    \"\"\"\n",
    "    Elimina la cabecera y opcionalmente la firma de un documento, usando criterios\n",
    "    específicos y conservadores para la detección de firmas.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    lineas = texto.split('\\n')\n",
    "    \n",
    "    # Buscamos la primera línea en blanco que separa la cabecera del cuerpo\n",
    "    indice_final_cabecera = -1\n",
    "    for i, linea in enumerate(lineas):\n",
    "        if linea.strip() == '' and i > 0:  # Aseguramos que no sea la primera línea\n",
    "            indice_final_cabecera = i\n",
    "            break\n",
    "    \n",
    "    # Si encontramos la línea en blanco, eliminamos la cabecera\n",
    "    if indice_final_cabecera != -1:\n",
    "        texto_sin_cabecera = '\\n'.join(lineas[indice_final_cabecera + 1:])\n",
    "    else:\n",
    "        texto_sin_cabecera = texto\n",
    "    \n",
    "    # Eliminación de firmas con criterios más específicos\n",
    "    lineas = texto_sin_cabecera.split('\\n')\n",
    "    \n",
    "    # Criterios para detectar inicio de firma\n",
    "    posibles_inicios_firma = []\n",
    "    \n",
    "    for i, linea in enumerate(lineas):\n",
    "        # Criterio 1: Líneas que son separadores claros\n",
    "        if linea.strip() in ['--', '---', '----'] or linea.startswith('-- ') or linea.startswith('- '):\n",
    "            posibles_inicios_firma.append(i)\n",
    "        \n",
    "        # Criterio 2: Líneas que contienen solo un correo electrónico\n",
    "        elif re.match(r'^\\s*[\\w\\.-]+@[\\w\\.-]+\\s*$', linea):\n",
    "            posibles_inicios_firma.append(i)\n",
    "    \n",
    "    # Si encontramos posibles inicios de firma\n",
    "    if posibles_inicios_firma:\n",
    "        # Tomamos el primer posible inicio\n",
    "        indice_inicio_firma = posibles_inicios_firma[0]\n",
    "        texto_sin_firma = '\\n'.join(lineas[:indice_inicio_firma])\n",
    "    else:\n",
    "        texto_sin_firma = texto_sin_cabecera\n",
    "    \n",
    "    return texto_sin_firma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bac83d5-e91e-41f4-9fb5-6b5179f6abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_y_procesar_documentos(ruta_base):\n",
    "    \"\"\"\n",
    "    Lee los archivos, elimina cabeceras y firmas, y devuelve un DataFrame\n",
    "    \"\"\"\n",
    "    archivos, categorias = listar_archivos(ruta_base)\n",
    "    \n",
    "    documentos = []\n",
    "    textos_originales = []\n",
    "    textos_procesados = []\n",
    "    \n",
    "    for archivo in archivos:\n",
    "        try:\n",
    "            with open(archivo, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                texto = f.read()\n",
    "            \n",
    "            textos_originales.append(texto)\n",
    "            \n",
    "            # Eliminamos cabecera y firma\n",
    "            texto_procesado = eliminar_cabecera_firma(texto)\n",
    "            textos_procesados.append(texto_procesado)\n",
    "            \n",
    "            documentos.append({\n",
    "                'archivo': archivo,\n",
    "                'categoria': os.path.basename(os.path.dirname(archivo)),\n",
    "                'texto_original': texto,\n",
    "                'texto_procesado': texto_procesado\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar archivo {archivo}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9024dd3e-15fb-430f-9277-fdf9d4cc307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_texto(texto, stemming=True):\n",
    "    \"\"\"\n",
    "    Realiza el preprocesamiento completo del texto:\n",
    "    1. Tokenización\n",
    "    2. Eliminación de palabras vacías\n",
    "    3. Stemming o lematización\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Tokenización usando el tokenizador ya descargado\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        tokens = word_tokenize(texto.lower())\n",
    "    except LookupError:\n",
    "        # Si hay algún problema con el tokenizador, usamos una alternativa simple\n",
    "        texto_lower = texto.lower()\n",
    "        # Eliminamos puntuación básica\n",
    "        for char in '.,;:!?\"()[]{}/-_':\n",
    "            texto_lower = texto_lower.replace(char, ' ')\n",
    "        # Tokenizamos por espacios\n",
    "        tokens = texto_lower.split()\n",
    "    \n",
    "    # Eliminación de signos de puntuación y caracteres especiales\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Eliminación de stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming o lematización\n",
    "    if stemming:\n",
    "        # Aplicamos stemming (Porter)\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    else:\n",
    "        # Aplicamos lematización (WordNet)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a059b179-8ff7-4232-85ef-711af19d208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_representacion_tf(documentos_procesados):\n",
    "    \"\"\"\n",
    "    Genera la representación TF para los documentos\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(documentos_procesados)\n",
    "    \n",
    "    # Convertimos a DataFrame para mejor visualización\n",
    "    df_tf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return df_tf, vectorizer\n",
    "\n",
    "def generar_representacion_tfidf(documentos_procesados):\n",
    "    \"\"\"\n",
    "    Genera la representación TF-IDF para los documentos\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(documentos_procesados)\n",
    "    \n",
    "    # Convertimos a DataFrame para mejor visualización\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return df_tfidf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5736441-381a-4da1-bf8b-6eb76577a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_modelo_word2vec(ruta_modelo=None):\n",
    "    \"\"\"\n",
    "    Carga un modelo Word2Vec preentrenado o crea uno simple si no se proporciona ruta\n",
    "    \"\"\"\n",
    "    if ruta_modelo and os.path.exists(ruta_modelo):\n",
    "        # Cargar modelo preentrenado desde una ruta específica\n",
    "        modelo = KeyedVectors.load_word2vec_format(ruta_modelo, binary=True)\n",
    "    else:\n",
    "        try:\n",
    "            # Intentar usar la importación correcta para el downloader\n",
    "            import gensim.downloader as api\n",
    "            try:\n",
    "                print(\"Intentando cargar 'glove-wiki-gigaword-50'...\")\n",
    "                modelo = api.load('glove-wiki-gigaword-50')  # Modelo más pequeño\n",
    "            except:\n",
    "                print(\"Intentando cargar 'word2vec-google-news-300'...\")\n",
    "                modelo = api.load('word2vec-google-news-300')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar modelos preentrenados: {e}\")\n",
    "            print(\"Creando un modelo con los documentos disponibles...\")\n",
    "            \n",
    "            # Extraemos las listas de tokens de los documentos\n",
    "            documentos_tokens = df_documentos['tokens'].tolist()\n",
    "            \n",
    "            # Entrenamos un modelo Word2Vec con nuestros documentos\n",
    "            modelo = Word2Vec(sentences=documentos_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "            \n",
    "            # Accedemos a los vectores\n",
    "            modelo = modelo.wv\n",
    "    \n",
    "    return modelo\n",
    "\n",
    "def generar_representacion_aditiva(documentos_tokens, modelo):\n",
    "    \"\"\"\n",
    "    Genera la representación aditiva para los documentos usando un modelo preentrenado\n",
    "    \"\"\"\n",
    "    representaciones = []\n",
    "    \n",
    "    for tokens in documentos_tokens:\n",
    "        # Filtramos solo palabras que están en el vocabulario del modelo\n",
    "        tokens_validos = [token for token in tokens if token in modelo.key_to_index]\n",
    "        \n",
    "        if tokens_validos:\n",
    "            # Modelo aditivo: sumamos los vectores de todas las palabras\n",
    "            vector_documento = sum(modelo[token] for token in tokens_validos)\n",
    "        else:\n",
    "            # Si no hay tokens válidos, usamos un vector de ceros\n",
    "            vector_documento = np.zeros(modelo.vector_size)\n",
    "        \n",
    "        representaciones.append(vector_documento)\n",
    "    \n",
    "    return np.array(representaciones)\n",
    "\n",
    "def generar_representacion_media(documentos_tokens, modelo):\n",
    "    \"\"\"\n",
    "    Genera la representación media para los documentos usando un modelo preentrenado\n",
    "    \"\"\"\n",
    "    representaciones = []\n",
    "    \n",
    "    for tokens in documentos_tokens:\n",
    "        # Filtramos solo palabras que están en el vocabulario del modelo\n",
    "        tokens_validos = [token for token in tokens if token in modelo.key_to_index]\n",
    "        \n",
    "        if tokens_validos:\n",
    "            # Modelo de la media: promediamos los vectores de todas las palabras\n",
    "            vector_documento = sum(modelo[token] for token in tokens_validos) / len(tokens_validos)\n",
    "        else:\n",
    "            # Si no hay tokens válidos, usamos un vector de ceros\n",
    "            vector_documento = np.zeros(modelo.vector_size)\n",
    "        \n",
    "        representaciones.append(vector_documento)\n",
    "    \n",
    "    return np.array(representaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd468325-2494-4888-9195-5d7eda62d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_representacion(representacion, nombre_archivo):\n",
    "    \"\"\"\n",
    "    Guarda la representación en un archivo de texto\n",
    "    \"\"\"\n",
    "    print(f\"Guardando en {nombre_archivo}...\")\n",
    "    \n",
    "    if isinstance(representacion, pd.DataFrame):\n",
    "        # Si es un DataFrame (como TF o TF-IDF)\n",
    "        representacion.to_csv(nombre_archivo, sep='\\t', index=False)\n",
    "    else:\n",
    "        # Si es un array (como las representaciones de word embeddings)\n",
    "        np.savetxt(nombre_archivo, representacion, delimiter='\\t')\n",
    "        \n",
    "    print(f\"Representación guardada en {nombre_archivo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7895fdd7-e3cf-4420-ba9e-3a29001a134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_proceso_completo(ruta_base, ruta_salida, usar_stemming=True):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso completo de generación de representaciones\n",
    "    \"\"\"\n",
    "    print(\"Leyendo y procesando documentos...\")\n",
    "    df_documentos = leer_y_procesar_documentos(ruta_base)\n",
    "    \n",
    "    print(f\"Se han procesado {len(df_documentos)} documentos de {df_documentos['categoria'].nunique()} categorías\")\n",
    "    \n",
    "    # Preprocesamiento para el modelo del espacio vectorial\n",
    "    print(\"Realizando preprocesamiento de textos...\")\n",
    "    df_documentos['tokens'] = df_documentos['texto_procesado'].apply(\n",
    "        lambda x: preprocesar_texto(x, stemming=usar_stemming)\n",
    "    )\n",
    "    \n",
    "    # Unimos los tokens para tener texto preprocesado\n",
    "    df_documentos['texto_preprocesado'] = df_documentos['tokens'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Generamos representaciones para el modelo del espacio vectorial\n",
    "    print(\"Generando representación TF...\")\n",
    "    df_tf, _ = generar_representacion_tf(df_documentos['texto_preprocesado'])\n",
    "    \n",
    "    print(\"Generando representación TF-IDF...\")\n",
    "    df_tfidf, _ = generar_representacion_tfidf(df_documentos['texto_preprocesado'])\n",
    "    \n",
    "    # Cargamos modelo preentrenado para el modelo semántico vectorial\n",
    "    print(\"Cargando modelo Word2Vec preentrenado...\")\n",
    "    modelo_w2v = cargar_modelo_word2vec()\n",
    "    \n",
    "    print(\"Generando representación aditiva...\")\n",
    "    representacion_aditiva = generar_representacion_aditiva(df_documentos['tokens'], modelo_w2v)\n",
    "    \n",
    "    print(\"Generando representación media...\")\n",
    "    representacion_media = generar_representacion_media(df_documentos['tokens'], modelo_w2v)\n",
    "    \n",
    "    # Guardamos las representaciones\n",
    "    print(\"Guardando representaciones...\")\n",
    "    guardar_representacion(df_tf, os.path.join(ruta_salida, 'representacion_tf.txt'))\n",
    "    guardar_representacion(df_tfidf, os.path.join(ruta_salida, 'representacion_tfidf.txt'))\n",
    "    guardar_representacion(representacion_aditiva, os.path.join(ruta_salida, 'representacion_aditiva.txt'))\n",
    "    guardar_representacion(representacion_media, os.path.join(ruta_salida, 'representacion_media.txt'))\n",
    "    \n",
    "    print(\"Proceso completado correctamente.\")\n",
    "    \n",
    "    return df_documentos, df_tf, df_tfidf, representacion_aditiva, representacion_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4efcebb0-e198-4a56-8523-b18df8a14d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo y procesando documentos...\n",
      "Se han procesado 805 documentos de 7 categorías\n",
      "Realizando preprocesamiento de textos...\n",
      "Generando representación TF...\n",
      "Generando representación TF-IDF...\n",
      "Cargando modelo Word2Vec preentrenado...\n",
      "Intentando cargar 'glove-wiki-gigaword-50'...\n",
      "[=================---------------------------------] 35.5% 23.4/66.0MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando representación aditiva...\n",
      "Generando representación media...\n",
      "Guardando representaciones...\n",
      "Guardando en ./resultados/representacion_tf.txt...\n",
      "Representación guardada en ./resultados/representacion_tf.txt\n",
      "Guardando en ./resultados/representacion_tfidf.txt...\n",
      "Representación guardada en ./resultados/representacion_tfidf.txt\n",
      "Guardando en ./resultados/representacion_aditiva.txt...\n",
      "Representación guardada en ./resultados/representacion_aditiva.txt\n",
      "Guardando en ./resultados/representacion_media.txt...\n",
      "Representación guardada en ./resultados/representacion_media.txt\n",
      "Proceso completado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Definimos las rutas de entrada y salida\n",
    "ruta_base = './Corpus-representacion'  # Carpeta con la ubicación de tus datos\n",
    "ruta_salida = './resultados'  # Carpeta donde se guardarán los resultados\n",
    "\n",
    "# Creamos la carpeta de salida si no existe\n",
    "if not os.path.exists(ruta_salida):\n",
    "    os.makedirs(ruta_salida)\n",
    "\n",
    "# Ejecutamos el proceso completo\n",
    "df_documentos, df_tf, df_tfidf, rep_aditiva, rep_media = ejecutar_proceso_completo(ruta_base, ruta_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0f6676b-2444-45a5-bfb8-a35a8206a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_evolucion_documento(df_documentos, indice=0):\n",
    "    \"\"\"\n",
    "    Analiza la evolución de un documento a lo largo del procesamiento,\n",
    "    con mejor visualización de la firma\n",
    "    \"\"\"\n",
    "    documento = df_documentos.iloc[indice]\n",
    "    \n",
    "    print(f\"Documento de la categoría: {documento['categoria']}\")\n",
    "    print(\"\\n----- TEXTO ORIGINAL (primeras 300 caracteres) -----\")\n",
    "    print(documento['texto_original'][:300])\n",
    "    \n",
    "    print(\"\\n----- TEXTO SIN CABECERA NI FIRMA (primeras 300 caracteres) -----\")\n",
    "    print(documento['texto_procesado'][:300])\n",
    "    \n",
    "    # Identificar mejor la posible firma\n",
    "    lineas_originales = documento['texto_original'].split('\\n')\n",
    "    texto_sin_cabecera = None\n",
    "    \n",
    "    # Identificar el final de la cabecera (primera línea en blanco)\n",
    "    for i, linea in enumerate(lineas_originales):\n",
    "        if linea.strip() == '' and i > 0:\n",
    "            texto_sin_cabecera = '\\n'.join(lineas_originales[i+1:])\n",
    "            break\n",
    "    \n",
    "    if texto_sin_cabecera:\n",
    "        lineas_sin_cabecera = texto_sin_cabecera.split('\\n')\n",
    "        \n",
    "        # Buscar posibles inicios de firma\n",
    "        posibles_inicios = []\n",
    "        for i, linea in enumerate(lineas_sin_cabecera):\n",
    "            if (linea.strip() in ['--', '---', '----'] or \n",
    "                linea.startswith('-- \\n') or \n",
    "                ('@' in linea and len(linea.split()) <= 3)):\n",
    "                posibles_inicios.append(i)\n",
    "        \n",
    "        if posibles_inicios:\n",
    "            indice_firma = posibles_inicios[0]\n",
    "            print(\"\\n----- POSIBLE FIRMA DETECTADA -----\")\n",
    "            print('\\n'.join(lineas_sin_cabecera[indice_firma:]))\n",
    "        else:\n",
    "            print(\"\\n----- NO SE DETECTÓ UNA FIRMA CLARA -----\")\n",
    "    else:\n",
    "        print(\"\\n----- NO SE PUDO IDENTIFICAR LA CABECERA -----\")\n",
    "    \n",
    "    # También mostramos final del texto procesado\n",
    "    print(\"\\n----- FINAL DEL TEXTO PROCESADO (últimas 5 líneas) -----\")\n",
    "    lineas_procesadas = documento['texto_procesado'].split('\\n')\n",
    "    for linea in lineas_procesadas[-5:]:\n",
    "        print(linea)\n",
    "    \n",
    "    print(\"\\n----- TOKENS DESPUÉS DEL PREPROCESAMIENTO (primeros 36) -----\")\n",
    "    print(documento['tokens'][:36])\n",
    "    \n",
    "    print(\"\\n----- TEXTO PREPROCESADO (primeros 100 caracteres) -----\")\n",
    "    print(documento['texto_preprocesado'][:100])\n",
    "    \n",
    "    # Análisis cuantitativo\n",
    "    print(\"\\n----- ANÁLISIS CUANTITATIVO -----\")\n",
    "    print(f\"Longitud del texto original: {len(documento['texto_original'])} caracteres\")\n",
    "    print(f\"Longitud del texto sin cabecera ni firma: {len(documento['texto_procesado'])} caracteres\")\n",
    "    print(f\"Número de tokens después del preprocesamiento: {len(documento['tokens'])}\")\n",
    "    \n",
    "    # Calculamos la reducción en porcentaje\n",
    "    reduccion_cabecera = 100 * (1 - len(documento['texto_procesado']) / len(documento['texto_original']))\n",
    "    reduccion_preprocesamiento = 100 * (1 - len(documento['tokens']) / len(documento['texto_procesado'].split()))\n",
    "    \n",
    "    print(f\"Reducción después de eliminar cabecera y firma: {reduccion_cabecera:.2f}%\")\n",
    "    print(f\"Reducción después del preprocesamiento: {reduccion_preprocesamiento:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e439221-7129-46d6-af47-12affe9227f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENTO ALEATORIO (índice 654):\n",
      "Documento de la categoría: sci.electronics\n",
      "\n",
      "----- TEXTO ORIGINAL (primeras 300 caracteres) -----\n",
      "Path: cantaloupe.srv.cs.cmu.edu!rochester!udel!gatech!europa.eng.gtefsd.com!howland.reston.ans.net!noc.near.net!uunet!pipex!uknet!uknet!cam-eng!cmh\n",
      "From: cmh@eng.cam.ac.uk (C.M. Hicks)\n",
      "Newsgroups: sci.electronics\n",
      "Subject: Re: MICROPHONE PRE-AMP/LOW NOISE/PHANTOM POWERED\n",
      "Message-ID: <1993Apr16.090648\n",
      "\n",
      "----- TEXTO SIN CABECERA NI FIRMA (primeras 300 caracteres) -----\n",
      "davidj@rahul.net (David Josephson) writes:\n",
      "\n",
      ">In <C5JJJ2.1tF@cmcl2.nyu.edu> ali@cns.nyu.edu (Alan Macaluso) writes:\n",
      "\n",
      ">>I'm looking to build a microphone preamp that has very good low-noise characteristics,  large clean gain, and incorportates phantom power (20-48 volts (dc)) for a PZM microphone.  I'\n",
      "\n",
      "----- POSIBLE FIRMA DETECTADA -----\n",
      "--\n",
      " ==============================================================================\n",
      "  Christopher Hicks    |      Paradise is a Linear Gaussian World\n",
      "  cmh@uk.ac.cam.eng    |    (also reported to taste hot and sweaty)\n",
      " ==============================================================================\n",
      "\n",
      "\n",
      "----- FINAL DEL TEXTO PROCESADO (últimas 5 líneas) -----\n",
      "I've had very good results from the SSM2016 from PMI (part of Analogue\n",
      "Devices). They have also now introduced the SSM2017 which looks good on\n",
      "paper, but which I haven't tried yet.\n",
      "\n",
      "Christopher\n",
      "\n",
      "----- TOKENS DESPUÉS DEL PREPROCESAMIENTO (primeros 36) -----\n",
      "['net', 'david', 'josephson', 'write', 'nyu', 'nyu', 'edu', 'alan', 'macaluso', 'write', 'look', 'build', 'microphon', 'preamp', 'good', 'low', 'nois', 'characterist', 'larg', 'clean', 'gain', 'incorport', 'phantom', 'power', 'volt', 'dc', 'pzm', 'microphon', 'lean', 'toward', 'good', 'low', 'cost', 'instrument', 'amplifi', 'maintain']\n",
      "\n",
      "----- TEXTO PREPROCESADO (primeros 100 caracteres) -----\n",
      "net david josephson write nyu nyu edu alan macaluso write look build microphon preamp good low nois \n",
      "\n",
      "----- ANÁLISIS CUANTITATIVO -----\n",
      "Longitud del texto original: 1819 caracteres\n",
      "Longitud del texto sin cabecera ni firma: 997 caracteres\n",
      "Número de tokens después del preprocesamiento: 83\n",
      "Reducción después de eliminar cabecera y firma: 45.19%\n",
      "Reducción después del preprocesamiento: 44.67%\n"
     ]
    }
   ],
   "source": [
    "# Analizamos la evolución de un documento ejemplo aleatoriamente\n",
    "import random\n",
    "indice_aleatorio = random.randint(0, len(df_documentos) - 1)\n",
    "print(f\"DOCUMENTO ALEATORIO (índice {indice_aleatorio}):\")\n",
    "analizar_evolucion_documento(df_documentos, indice=indice_aleatorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b3e7b5c-9e6f-4a3c-a84a-fd1a66944472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENTO ALEATORIO (índice 79):\n",
      "Documento de la categoría: rec.autos\n",
      "\n",
      "----- TEXTO ORIGINAL (primeras 300 caracteres) -----\n",
      "Newsgroups: rec.autos\n",
      "Path: cantaloupe.srv.cs.cmu.edu!rochester!cornell!batcomputer!caen!spool.mu.edu!torn!watserv2.uwaterloo.ca!watserv1!mks.com!mike\n",
      "From: mike@mks.com (Mike Brookbank)\n",
      "Subject: MGBs and the real world\n",
      "Message-ID: <1993Apr5.181056.29411@mks.com>\n",
      "Organization: Mortice Kern Systems I\n",
      "\n",
      "----- TEXTO SIN CABECERA NI FIRMA (primeras 300 caracteres) -----\n",
      "My sister has an MGB.  She has one from the last year they were produced\n",
      "(1978? 1979?).  Its in very good shape.  I've been bugging her for years\n",
      "about selling it.  I've said over and over that she should sell it\n",
      "before the car is worthless while she maintains that the car may\n",
      "actually be increasing\n",
      "\n",
      "----- POSIBLE FIRMA DETECTADA -----\n",
      "-- \n",
      "------------------------------------------------------------------------------\n",
      "Mike Brookbank,                 |MKS| 35 King St. North       mike@mks.com \n",
      "Director, InterOpen Sales,      |MKT| Waterloo, Ontario      (519)884-2251 \n",
      "Mortice Kern Systems Inc.       |MKS| Canada, N2J 2W9    fax (519)884-8861\n",
      "\n",
      "\n",
      "----- FINAL DEL TEXTO PROCESADO (últimas 5 líneas) -----\n",
      "actually be increasing in value as a result of its limited availability.\n",
      "\n",
      "Which one of us is right?  Are there MGB affectionados out there who are\n",
      "still willing to pay $6K to 8K for an old MG?  Are there a lot out in the \n",
      "market?\n",
      "\n",
      "----- TOKENS DESPUÉS DEL PREPROCESAMIENTO (primeros 36) -----\n",
      "['sister', 'mgb', 'one', 'last', 'year', 'produc', 'good', 'shape', 'bug', 'year', 'sell', 'said', 'sell', 'car', 'worthless', 'maintain', 'car', 'may', 'actual', 'increas', 'valu', 'result', 'limit', 'avail', 'one', 'us', 'right', 'mgb', 'affectionado', 'still', 'will', 'pay', 'old', 'mg', 'lot', 'market']\n",
      "\n",
      "----- TEXTO PREPROCESADO (primeros 100 caracteres) -----\n",
      "sister mgb one last year produc good shape bug year sell said sell car worthless maintain car may ac\n",
      "\n",
      "----- ANÁLISIS CUANTITATIVO -----\n",
      "Longitud del texto original: 1195 caracteres\n",
      "Longitud del texto sin cabecera ni firma: 507 caracteres\n",
      "Número de tokens después del preprocesamiento: 36\n",
      "Reducción después de eliminar cabecera y firma: 57.57%\n",
      "Reducción después del preprocesamiento: 63.27%\n"
     ]
    }
   ],
   "source": [
    "# Analizamos la evolución de un documento ejemplo (79) para el informe\n",
    "print(f\"DOCUMENTO ALEATORIO (índice {79}):\")\n",
    "analizar_evolucion_documento(df_documentos, indice=79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12e7013b-414f-4f43-b997-2e04e8e427ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_y_guardar_visualizaciones(df_documentos, df_tf, df_tfidf, rep_aditiva, rep_media, ruta_salida):\n",
    "    \"\"\"\n",
    "    Genera y guarda visualizaciones adicionales para el análisis de las representaciones\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    # Crear la carpeta para gráficas si no existe\n",
    "    ruta_graficas = os.path.join(ruta_salida, 'graficas')\n",
    "    if not os.path.exists(ruta_graficas):\n",
    "        os.makedirs(ruta_graficas)\n",
    "    \n",
    "    # 1. Longitud media de documentos por categoría\n",
    "\n",
    "    # Calculamos los datos para el gráfico\n",
    "    datos_longitud = df_documentos.groupby('categoria')['tokens'].apply(lambda x: np.mean([len(tokens) for tokens in x]))\n",
    "    \n",
    "    # Función para personalizar nombres de categorías\n",
    "    def acortar_categoria(categoria, max_len=18):\n",
    "        # Acortamos categorías específicas según reglas personalizadas\n",
    "        if len(categoria) > max_len:\n",
    "            return categoria[:max_len-3] + \"...\"\n",
    "        else:\n",
    "            return categoria\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    valores = datos_longitud.values\n",
    "    categorias = [acortar_categoria(cat) for cat in datos_longitud.index]\n",
    "    plt.bar(range(len(valores)), valores)\n",
    "    plt.xticks(range(len(categorias)), categorias, rotation=0, fontsize=9)\n",
    "    plt.ylabel('Número medio de tokens')\n",
    "    plt.xlabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ruta_graficas, 'longitud_por_categoria.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Palabras más frecuentes\n",
    "    todas_palabras = [token for tokens in df_documentos['tokens'] for token in tokens]\n",
    "    contador = Counter(todas_palabras)\n",
    "    palabras_comunes = contador.most_common(20)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=[palabra for palabra, _ in palabras_comunes], \n",
    "                y=[frecuencia for _, frecuencia in palabras_comunes])\n",
    "    # plt.title('20 palabras más frecuentes en la colección')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ruta_graficas, 'palabras_frecuentes.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Dimensionalidad de las representaciones\n",
    "    dimensiones = {\n",
    "        'TF/TF-IDF': len(df_tf.columns),\n",
    "        'Word Embeddings': rep_aditiva.shape[1]\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(dimensiones.keys(), dimensiones.values())\n",
    "    # plt.title('Dimensionalidad de las representaciones')\n",
    "    plt.ylabel('Número de dimensiones')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ruta_graficas, 'dimensionalidad.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. NUEVA: Dispersión de las representaciones\n",
    "    # Calculamos el porcentaje de ceros en TF y TF-IDF\n",
    "    sparsity_tf = (df_tf.values == 0).sum() / df_tf.size * 100\n",
    "    sparsity_tfidf = (df_tfidf.values == 0).sum() / df_tfidf.size * 100\n",
    "    sparsity_embed = (rep_aditiva == 0).sum() / rep_aditiva.size * 100\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(['TF', 'TF-IDF', 'Word Embeddings'], [sparsity_tf, sparsity_tfidf, sparsity_embed])\n",
    "    # plt.title('Dispersión de las representaciones')\n",
    "    plt.ylabel('Porcentaje de valores cero (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ruta_graficas, 'dispersion.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. NUEVA: Visualización de agrupamiento con PCA\n",
    "    # Seleccionamos un subconjunto para visualización (para mayor rapidez)\n",
    "    muestra = min(500, len(df_documentos))\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.choice(len(df_documentos), muestra, replace=False)\n",
    "    \n",
    "    # Aplicamos PCA a las diferentes representaciones\n",
    "    try:\n",
    "        # Para TF-IDF\n",
    "        pca_tfidf = PCA(n_components=2, random_state=42).fit_transform(df_tfidf.iloc[indices].values)\n",
    "        \n",
    "        # Para embeddings aditivos\n",
    "        pca_embed = PCA(n_components=2, random_state=42).fit_transform(rep_aditiva[indices])\n",
    "        \n",
    "        # Categorías para colorear\n",
    "        categorias = df_documentos.iloc[indices]['categoria'].values\n",
    "        \n",
    "        # Visualización de PCA para TF-IDF\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for categoria in np.unique(categorias):\n",
    "            indices_cat = [i for i, cat in enumerate(categorias) if cat == categoria]\n",
    "            plt.scatter(pca_tfidf[indices_cat, 0], pca_tfidf[indices_cat, 1], label=categoria, alpha=0.7)\n",
    "        #plt.title('Visualización PCA de representaciones TF-IDF')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(ruta_graficas, 'pca_tfidf.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Visualización de PCA para embeddings\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for categoria in np.unique(categorias):\n",
    "            indices_cat = [i for i, cat in enumerate(categorias) if cat == categoria]\n",
    "            plt.scatter(pca_embed[indices_cat, 0], pca_embed[indices_cat, 1], label=categoria, alpha=0.7)\n",
    "        # plt.title('Visualización PCA de representaciones de Word Embeddings')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(ruta_graficas, 'pca_embeddings.png'))\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar visualizaciones PCA: {e}\")\n",
    "    \n",
    "    # 6. NUEVA: Matriz de similaridad para documentos de ejemplo\n",
    "    try:\n",
    "        # Seleccionamos algunos documentos de ejemplo (uno de cada categoría)\n",
    "        categorias_unicas = df_documentos['categoria'].unique()\n",
    "        indices_ejemplo = [df_documentos[df_documentos['categoria'] == cat].index[0] for cat in categorias_unicas]\n",
    "        \n",
    "        # Calculamos similaridad coseno entre estos documentos usando TF-IDF\n",
    "        sim_tfidf = cosine_similarity(df_tfidf.iloc[indices_ejemplo])\n",
    "        \n",
    "        # Calculamos similaridad coseno usando embeddings\n",
    "        sim_embed = cosine_similarity(rep_aditiva[indices_ejemplo])\n",
    "        \n",
    "        # Visualizamos las matrices de similaridad\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(sim_tfidf, annot=True, cmap='Blues', xticklabels=categorias_unicas, yticklabels=categorias_unicas)\n",
    "        # plt.title('Similaridad coseno entre documentos (TF-IDF)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(ruta_graficas, 'similaridad_tfidf.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(sim_embed, annot=True, cmap='Blues', xticklabels=categorias_unicas, yticklabels=categorias_unicas)\n",
    "        # plt.title('Similaridad coseno entre documentos (Word Embeddings)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(ruta_graficas, 'similaridad_embeddings.png'))\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar matrices de similaridad: {e}\")\n",
    "    \n",
    "    print(f\"Visualizaciones guardadas en {ruta_graficas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8abd7207-7c77-4a12-b18a-f36fd64c4188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo y procesando documentos...\n",
      "Se han procesado 805 documentos de 7 categorías\n",
      "Realizando preprocesamiento de textos...\n",
      "Generando representación TF...\n",
      "Generando representación TF-IDF...\n",
      "Cargando modelo Word2Vec preentrenado...\n",
      "Intentando cargar 'glove-wiki-gigaword-50'...\n",
      "Generando representación aditiva...\n",
      "Generando representación media...\n",
      "Guardando representaciones...\n",
      "Guardando en ./resultados/representacion_tf.txt...\n",
      "Representación guardada en ./resultados/representacion_tf.txt\n",
      "Guardando en ./resultados/representacion_tfidf.txt...\n",
      "Representación guardada en ./resultados/representacion_tfidf.txt\n",
      "Guardando en ./resultados/representacion_aditiva.txt...\n",
      "Representación guardada en ./resultados/representacion_aditiva.txt\n",
      "Guardando en ./resultados/representacion_media.txt...\n",
      "Representación guardada en ./resultados/representacion_media.txt\n",
      "Proceso completado correctamente.\n",
      "Visualizaciones guardadas en ./resultados/graficas\n"
     ]
    }
   ],
   "source": [
    "# Después de ejecutar el proceso completo y obtener las representaciones\n",
    "df_documentos, df_tf, df_tfidf, rep_aditiva, rep_media = ejecutar_proceso_completo(ruta_base, ruta_salida)\n",
    "\n",
    "# Generar y guardar visualizaciones\n",
    "generar_y_guardar_visualizaciones(df_documentos, df_tf, df_tfidf, rep_aditiva, rep_media, ruta_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d42d53d-15e0-43d8-ab0a-56472aca6323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis complementario: comparación entre truncamiento (stemming) y lematización\n"
     ]
    }
   ],
   "source": [
    "# Análisis complementario: impacto de la lematización en modelos semánticos vectoriales\n",
    "print(\"Análisis complementario: comparación entre truncamiento (stemming) y lematización\")\n",
    "\n",
    "def preprocesar_texto_con_lematizacion(texto):\n",
    "    \"\"\"\n",
    "    Realiza el preprocesamiento completo del texto usando lematización en lugar de stemming:\n",
    "    1. Tokenización\n",
    "    2. Eliminación de palabras vacías\n",
    "    3. Lematización\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Tokenización usando el tokenizador NLTK\n",
    "        tokens = word_tokenize(texto.lower())\n",
    "    except LookupError:\n",
    "        # Si hay algún problema con el tokenizador, usamos una alternativa simple\n",
    "        texto_lower = texto.lower()\n",
    "        # Eliminamos puntuación básica\n",
    "        for char in '.,;:!?\"()[]{}/-_':\n",
    "            texto_lower = texto_lower.replace(char, ' ')\n",
    "        # Tokenizamos por espacios\n",
    "        tokens = texto_lower.split()\n",
    "    \n",
    "    # Eliminación de signos de puntuación y caracteres especiales\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Eliminación de stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Aplicamos lematización en lugar de stemming\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "761a61b7-b71c-4f2b-ba1f-cc1f02df3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos el vocabulario resultante con stemming vs lematización\n",
    "def comparar_vocabularios(df_documentos):\n",
    "    \"\"\"\n",
    "    Compara el vocabulario resultante usando stemming vs lematización\n",
    "    \"\"\"\n",
    "    # Procesamos los documentos con lematización\n",
    "    print(\"Procesando documentos con lematización...\")\n",
    "    df_documentos['tokens_lematizados'] = df_documentos['texto_procesado'].apply(\n",
    "        lambda x: preprocesar_texto_con_lematizacion(x)\n",
    "    )\n",
    "    \n",
    "    # Obtenemos los vocabularios\n",
    "    vocab_stemming = set(token for tokens in df_documentos['tokens'] for token in tokens)\n",
    "    vocab_lematizacion = set(token for tokens in df_documentos['tokens_lematizados'] for token in tokens)\n",
    "    \n",
    "    # Calculamos estadísticas\n",
    "    print(f\"Tamaño del vocabulario con stemming: {len(vocab_stemming)}\")\n",
    "    print(f\"Tamaño del vocabulario con lematización: {len(vocab_lematizacion)}\")\n",
    "    \n",
    "    # Intersección y diferencias\n",
    "    interseccion = vocab_stemming.intersection(vocab_lematizacion)\n",
    "    solo_stemming = vocab_stemming - vocab_lematizacion\n",
    "    solo_lematizacion = vocab_lematizacion - vocab_stemming\n",
    "    \n",
    "    print(f\"Términos comunes a ambos vocabularios: {len(interseccion)} ({len(interseccion)/len(vocab_stemming)*100:.1f}% del vocabulario con stemming)\")\n",
    "    print(f\"Términos únicos de stemming: {len(solo_stemming)}\")\n",
    "    print(f\"Términos únicos de lematización: {len(solo_lematizacion)}\")\n",
    "    \n",
    "    # Mostramos algunos ejemplos de términos diferentes\n",
    "    print(\"\\nEjemplos de términos con stemming que no aparecen en lematización:\")\n",
    "    for term in list(solo_stemming)[:10]:\n",
    "        print(f\"  - {term}\")\n",
    "    \n",
    "    print(\"\\nEjemplos de términos con lematización que no aparecen en stemming:\")\n",
    "    for term in list(solo_lematizacion)[:10]:\n",
    "        print(f\"  - {term}\")\n",
    "    \n",
    "    return vocab_stemming, vocab_lematizacion, interseccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a9f33a5-923e-4a15-bef3-f5839ac35c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos representaciones basadas en word embeddings con lematización\n",
    "def generar_representaciones_con_lematizacion(df_documentos, modelo_w2v):\n",
    "    \"\"\"\n",
    "    Genera representaciones aditiva y media usando lematización\n",
    "    \"\"\"\n",
    "    # Generamos la representación aditiva\n",
    "    print(\"Generando representación aditiva con lematización...\")\n",
    "    representacion_aditiva_lem = generar_representacion_aditiva(df_documentos['tokens_lematizados'], modelo_w2v)\n",
    "    \n",
    "    # Generamos la representación media\n",
    "    print(\"Generando representación media con lematización...\")\n",
    "    representacion_media_lem = generar_representacion_media(df_documentos['tokens_lematizados'], modelo_w2v)\n",
    "    \n",
    "    # Guardamos las representaciones\n",
    "    print(\"Guardando representaciones...\")\n",
    "    guardar_representacion(representacion_aditiva_lem, os.path.join(ruta_salida, 'representacion_aditiva_lematizacion.txt'))\n",
    "    guardar_representacion(representacion_media_lem, os.path.join(ruta_salida, 'representacion_media_lematizacion.txt'))\n",
    "    \n",
    "    return representacion_aditiva_lem, representacion_media_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8617b56-7ab8-4963-992b-92deda04f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizamos la cobertura léxica en el modelo preentrenado\n",
    "def analizar_cobertura_lexica(df_documentos, modelo_w2v):\n",
    "    \"\"\"\n",
    "    Analiza cuántos tokens del vocabulario están presentes en el modelo preentrenado\n",
    "    \"\"\"\n",
    "    # Cobertura con stemming\n",
    "    tokens_stemming = [token for tokens in df_documentos['tokens'] for token in tokens]\n",
    "    tokens_unicos_stemming = set(tokens_stemming)\n",
    "    tokens_en_modelo_stemming = sum(1 for token in tokens_unicos_stemming if token in modelo_w2v.key_to_index)\n",
    "    \n",
    "    # Cobertura con lematización\n",
    "    tokens_lematizacion = [token for tokens in df_documentos['tokens_lematizados'] for token in tokens]\n",
    "    tokens_unicos_lematizacion = set(tokens_lematizacion)\n",
    "    tokens_en_modelo_lematizacion = sum(1 for token in tokens_unicos_lematizacion if token in modelo_w2v.key_to_index)\n",
    "    \n",
    "    # Calculamos porcentajes\n",
    "    cobertura_stemming = tokens_en_modelo_stemming / len(tokens_unicos_stemming) * 100\n",
    "    cobertura_lematizacion = tokens_en_modelo_lematizacion / len(tokens_unicos_lematizacion) * 100\n",
    "    \n",
    "    print(f\"Cobertura léxica con stemming: {tokens_en_modelo_stemming}/{len(tokens_unicos_stemming)} ({cobertura_stemming:.2f}%)\")\n",
    "    print(f\"Cobertura léxica con lematización: {tokens_en_modelo_lematizacion}/{len(tokens_unicos_lematizacion)} ({cobertura_lematizacion:.2f}%)\")\n",
    "    print(f\"Mejora en cobertura léxica: {cobertura_lematizacion - cobertura_stemming:.2f} puntos porcentuales\")\n",
    "    \n",
    "    return cobertura_stemming, cobertura_lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7499bebd-1414-47eb-8d1b-a3208c9558d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando documentos con lematización...\n",
      "Tamaño del vocabulario con stemming: 10044\n",
      "Tamaño del vocabulario con lematización: 12553\n",
      "Términos comunes a ambos vocabularios: 6518 (64.9% del vocabulario con stemming)\n",
      "Términos únicos de stemming: 3526\n",
      "Términos únicos de lematización: 6035\n",
      "\n",
      "Ejemplos de términos con stemming que no aparecen en lematización:\n",
      "  - overtak\n",
      "  - transmitt\n",
      "  - gover\n",
      "  - tennesse\n",
      "  - upstair\n",
      "  - roommat\n",
      "  - makalel\n",
      "  - machin\n",
      "  - doubl\n",
      "  - uncontitut\n",
      "\n",
      "Ejemplos de términos con lematización que no aparecen en stemming:\n",
      "  - semitic\n",
      "  - rising\n",
      "  - hades\n",
      "  - adobe\n",
      "  - federation\n",
      "  - symmetry\n",
      "  - pas\n",
      "  - caput\n",
      "  - dumped\n",
      "  - fairy\n",
      "\n",
      "Cargando modelo Word2Vec preentrenado...\n",
      "Intentando cargar 'glove-wiki-gigaword-50'...\n",
      "\n",
      "Analizando cobertura léxica en el modelo preentrenado...\n",
      "Cobertura léxica con stemming: 6710/10044 (66.81%)\n",
      "Cobertura léxica con lematización: 11185/12553 (89.10%)\n",
      "Mejora en cobertura léxica: 22.30 puntos porcentuales\n",
      "Generando representación aditiva con lematización...\n",
      "Generando representación media con lematización...\n",
      "Guardando representaciones...\n",
      "Guardando en ./resultados/representacion_aditiva_lematizacion.txt...\n",
      "Representación guardada en ./resultados/representacion_aditiva_lematizacion.txt\n",
      "Guardando en ./resultados/representacion_media_lematizacion.txt...\n",
      "Representación guardada en ./resultados/representacion_media_lematizacion.txt\n",
      "\n",
      "Análisis complementario completado. Las nuevas representaciones basadas en lematización han sido guardadas.\n"
     ]
    }
   ],
   "source": [
    "# Ejecutamos el análisis comparativo\n",
    "vocab_stemming, vocab_lematizacion, interseccion = comparar_vocabularios(df_documentos)\n",
    "\n",
    "# Cargamos modelo preentrenado\n",
    "print(\"\\nCargando modelo Word2Vec preentrenado...\")\n",
    "modelo_w2v = cargar_modelo_word2vec()\n",
    "\n",
    "# Analizamos la cobertura léxica\n",
    "print(\"\\nAnalizando cobertura léxica en el modelo preentrenado...\")\n",
    "cobertura_stemming, cobertura_lematizacion = analizar_cobertura_lexica(df_documentos, modelo_w2v)\n",
    "\n",
    "# Generamos las nuevas representaciones con lematización\n",
    "representacion_aditiva_lem, representacion_media_lem = generar_representaciones_con_lematizacion(df_documentos, modelo_w2v)\n",
    "\n",
    "print(\"\\nAnálisis complementario completado. Las nuevas representaciones basadas en lematización han sido guardadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9f2647-379c-432d-b829-6cce10da6815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
